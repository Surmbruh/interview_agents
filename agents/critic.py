"""
Critic Node - Quality control for Interviewer's questions.
Validates questions against repetition, grade alignment, and tone.
"""
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from state import AgentState, CriticOutput
from config import settings
from utils.llm_utils import llm_retry
from utils.log_config import get_logger

logger = get_logger("critic")


@llm_retry
def critic_node(state: AgentState):
    """
    Quality Critic Node: Validates the Interviewer's generated question.
    Checks for repetitions, grade alignment, and tone.
    """
    messages = state["messages"]
    candidate_info = state["candidate_info"]
    history = "\n".join([f"{m.type}: {m.content}" for m in messages[:-1]])
    last_question = messages[-1].content
    
    logger.debug("Validating question: %s", last_question[:80])
    
    # Initialize LLM using settings
    api_base = settings.OPENAI_API_BASE
    llm = ChatOpenAI(
        model=settings.MODEL_ROUTER, 
        temperature=0, 
        base_url=api_base,
        api_key=settings.OPENAI_API_KEY
    )
    
    # Structured output
    structured_llm = llm.with_structured_output(CriticOutput)
    
    system_prompt = """You are a Quality Control Agent for an Interview Coach.
Your task is to judge the LATEST QUESTION generated by the Interviewer.

## CRITERIA:
1. **NO REPETITION**: Is this question identical or very similar to a previous one in the history?
2. **GRADE ALIGNMENT**: Is the complexity appropriate for the candidate's grade ({grade})?
   - Senior: Focus on architecture, trade-offs, internal mechanics.
   - Junior: Focus on syntax, definitions, basic usage.
3. **TONE**: Is it polite, professional, and in Russian?
4. **CLARITY**: Is the question clear and unambiguous?

If it passes all criteria, set status to APPROVED.
If any criterion fails, set status to REJECTED and provide specific feedback on what to fix.
**You MUST write in Russian. DO NOT use emojis/emoticons.**
"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Conversation History:\n{history}\n\nLATEST QUESTION TO VERIFY:\n{last_question}")
    ])
    
    chain = prompt | structured_llm
    
    response: CriticOutput = chain.invoke({
        "grade": candidate_info.get("Grade", "Middle"),
        "history": history,
        "last_question": last_question
    })
    
    logger.info("Decision: %s", response.status)
    if response.status == "REJECTED":
        logger.debug("Feedback: %s", response.feedback)
    
    # Store thoughts
    critic_thought = f"Decision: {response.status}. {response.feedback}"
    current_thoughts = state.get("current_turn_thoughts", {})
    current_thoughts["Critic"] = critic_thought
        
    return {
        "critic_status": response.status,
        "critic_feedback": response.feedback,
        "current_turn_thoughts": current_thoughts
    }
